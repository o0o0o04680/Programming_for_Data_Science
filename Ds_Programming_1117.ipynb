{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc71231-91a9-4a1e-8776-f6d1de866519",
   "metadata": {},
   "source": [
    "!pip install fredapi\n",
    "\n",
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ee9ab-c09f-4885-9982-aba031313ff5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46427342-7298-4ab9-a8b5-f36f039a5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc13a4d-1c7b-488d-b991-e88d8adb319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key = \"ed1b2123fc23b6e2a2d8db120bb68b7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae520e-91bb-4486-b0d0-e0b767da1e59",
   "metadata": {},
   "source": [
    "Variable Name:\n",
    "\n",
    "Initial claim (ICSA) (Weekly) : A claim filed by an unemployed individual after a separation from an employer. The claim requests a determination of basic eligibility for the Unemployment Insurance program.\n",
    "\n",
    "cpi (CPIAUCSL) (Monthly):\n",
    "\n",
    "term_srpead (T10Y3M) (Daily): 10-Year Treasury Yield minus 3-Month Treasury Yield\n",
    "\n",
    "dgs10 (DGS10) (Daily): 10-Year Treasury Constant Maturity Rate\n",
    "\n",
    "credit_spred (BAA10YM) (Monthly): Corporate Bond Yield minus 10-Year Treasury Yield \n",
    "\n",
    "sp500 (^GDPC) (Daily):\n",
    "\n",
    "SGIXSENT (Daily): Sentiment Index\n",
    "\n",
    "VIX :\n",
    "\n",
    "VIX_retrun :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d38f46c-2607-47a2-be75-421a3ce96af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            initial_claims      cpi  term_spread  dgs10  credit_spread\n",
      "2018-01-01             NaN  248.859          NaN    NaN           1.68\n",
      "2018-01-02             NaN      NaN         1.02   2.46            NaN\n",
      "2018-01-03             NaN      NaN         1.03   2.44            NaN\n",
      "2018-01-04             NaN      NaN         1.05   2.46            NaN\n",
      "2018-01-05             NaN      NaN         1.08   2.47            NaN\n",
      "2018-01-06        252000.0      NaN          NaN    NaN            NaN\n",
      "2018-01-07             NaN      NaN          NaN    NaN            NaN\n",
      "2018-01-08             NaN      NaN         1.04   2.49            NaN\n",
      "2018-01-09             NaN      NaN         1.11   2.55            NaN\n",
      "2018-01-10             NaN      NaN         1.13   2.55            NaN\n",
      "2018-01-11             NaN      NaN         1.11   2.54            NaN\n",
      "2018-01-12             NaN      NaN         1.12   2.55            NaN\n",
      "2018-01-13        230000.0      NaN          NaN    NaN            NaN\n",
      "2018-01-14             NaN      NaN          NaN    NaN            NaN\n",
      "2018-01-15             NaN      NaN          NaN    NaN            NaN\n",
      "2018-01-16             NaN      NaN         1.09   2.54            NaN\n",
      "2018-01-17             NaN      NaN         1.13   2.57            NaN\n",
      "2018-01-18             NaN      NaN         1.17   2.62            NaN\n",
      "2018-01-19             NaN      NaN         1.20   2.64            NaN\n",
      "2018-01-20        231000.0      NaN          NaN    NaN            NaN\n",
      "2018-01-21             NaN      NaN          NaN    NaN            NaN\n",
      "2018-01-22             NaN      NaN         1.22   2.66            NaN\n",
      "2018-01-23             NaN      NaN         1.19   2.63            NaN\n",
      "2018-01-24             NaN      NaN         1.22   2.65            NaN\n",
      "2018-01-25             NaN      NaN         1.21   2.63            NaN\n",
      "2018-01-26             NaN      NaN         1.25   2.66            NaN\n",
      "2018-01-27        228000.0      NaN          NaN    NaN            NaN\n",
      "2018-01-28             NaN      NaN          NaN    NaN            NaN\n",
      "2018-01-29             NaN      NaN         1.26   2.70            NaN\n",
      "2018-01-30             NaN      NaN         1.29   2.73            NaN\n"
     ]
    }
   ],
   "source": [
    "series_ids = {\n",
    "    \"initial_claims\": \"ICSA\",\n",
    "    \"cpi\": \"CPIAUCSL\",\n",
    "    \"term_spread\": \"T10Y3M\",\n",
    "    \"dgs10\": \"DGS10\",\n",
    "    \"credit_spread\": \"BAA10YM\",\n",
    "} \n",
    "data = {}\n",
    "for name, sid in series_ids.items():\n",
    "    s = fred.get_series(sid)\n",
    "    s.name = name \n",
    "    data[name] = fred.get_series(sid)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df = df[(df.index >= \"2018-01-01\") & (df.index <= \"2025-10-30\")]\n",
    "full_index = pd.date_range(start=\"2018-01-01\", end=\"2025-10-30\", freq=\"D\")\n",
    "df = df.reindex(full_index)\n",
    "print(df.head(30)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f76149-1c3b-43ab-b25f-c7dda147dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22788\\2462918612.py:1: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  sp500 = yf.download(\"^GSPC\", start=\"2018-01-01\", end=\"2025-10-30\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker            ^GSPC\n",
      "Date                   \n",
      "2018-01-02  2695.810059\n",
      "2018-01-03  2713.060059\n",
      "2018-01-04  2723.989990\n",
      "2018-01-05  2743.149902\n",
      "2018-01-08  2747.709961 \n",
      "\n",
      "Time Periods： 2018-01-02 00:00:00 ~ 2025-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sp500 = yf.download(\"^GSPC\", start=\"2018-01-01\", end=\"2025-10-30\")\n",
    "sp500 = sp500[\"Close\"]\n",
    "sp500.name = \"sp500\"\n",
    "print(sp500.head(), \"\\n\")\n",
    "print(\"Time Periods：\", sp500.index.min(), \"~\", sp500.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f71ff0-a4b2-4aaa-bba3-89bffc9f20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SGIXSENT\n",
      "Date                \n",
      "2014-01-15    243.05\n",
      "2014-01-16    243.28\n",
      "2014-01-17    244.07\n",
      "2014-01-18       NaN\n",
      "2014-01-19       NaN\n",
      "2014-01-20       NaN\n",
      "2014-01-21    244.22\n",
      "2014-01-22    243.29\n",
      "2014-01-23    243.48\n",
      "2014-01-24    241.80\n",
      "            SGIXSENT\n",
      "Date                \n",
      "2025-10-21    353.69\n",
      "2025-10-22    352.76\n",
      "2025-10-23    351.30\n",
      "2025-10-24    350.91\n",
      "2025-10-25       NaN\n",
      "2025-10-26       NaN\n",
      "2025-10-27    351.19\n",
      "2025-10-28    352.30\n",
      "2025-10-29    351.29\n",
      "2025-10-30    349.96\n"
     ]
    }
   ],
   "source": [
    "df_sent = pd.read_excel(\"SGIXSENT_INDEX_20040115-20252030.xlsx\",\n",
    "                   sheet_name=[\"20140115_20240115\", \"20240115_20251030\"])\n",
    "df1 = df_sent[\"20140115_20240115\"]\n",
    "df2 = df_sent[\"20240115_20251030\"]\n",
    "def clean_sentiment(df):\n",
    "    df = df.iloc[1:, 1:3].copy()\n",
    "    df.columns = [\"SGIXSENT\", \"Date\"]\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"SGIXSENT\"] = pd.to_numeric(df[\"SGIXSENT\"], errors=\"coerce\")\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    return df\n",
    "\n",
    "df_sent1 = clean_sentiment(df_sent[\"20140115_20240115\"])\n",
    "df_sent2 = clean_sentiment(df_sent[\"20240115_20251030\"])\n",
    "df_sent = pd.concat([df_sent1, df_sent2]).sort_index()\n",
    "print(df_sent.head(10))\n",
    "print(df_sent.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50b3eb9-2cbc-4fa6-8ad0-e0cc96cdfa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SGIXSENT\n",
      "Date                \n",
      "2018-01-01       NaN\n",
      "2018-01-02    315.61\n",
      "2018-01-03    315.61\n",
      "2018-01-04    317.07\n",
      "2018-01-05    318.38\n",
      "            SGIXSENT\n",
      "Date                \n",
      "2025-10-26       NaN\n",
      "2025-10-27    351.19\n",
      "2025-10-28    352.30\n",
      "2025-10-29    351.29\n",
      "2025-10-30    349.96\n"
     ]
    }
   ],
   "source": [
    "df_sent = df_sent.loc[\"2018-01-01\" : \"2025-10-30\"]\n",
    "print(df_sent.head())\n",
    "print(df_sent.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfa3301-d4f3-46a1-bd2a-d829fc00dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VIX\n",
      "2018-01-01    NaN\n",
      "2018-01-02   9.77\n",
      "2018-01-03   9.15\n",
      "2018-01-04   9.22\n",
      "2018-01-05   9.22\n",
      "2018-01-08   9.52\n",
      "2018-01-09  10.08\n",
      "2018-01-10   9.82\n",
      "2018-01-11   9.88\n",
      "2018-01-12  10.16\n",
      "              VIX\n",
      "2025-10-17  20.78\n",
      "2025-10-20  18.23\n",
      "2025-10-21  17.87\n",
      "2025-10-22  18.60\n",
      "2025-10-23  17.30\n",
      "2025-10-24  16.37\n",
      "2025-10-27  15.79\n",
      "2025-10-28  16.42\n",
      "2025-10-29  16.92\n",
      "2025-10-30  16.91\n"
     ]
    }
   ],
   "source": [
    "vix = fred.get_series(\n",
    "    \"VIXCLS\",\n",
    "    observation_start=\"2018-01-01\",\n",
    "    observation_end=\"2025-10-30\"\n",
    ")\n",
    "df_vix = pd.DataFrame(vix, columns=[\"VIX\"])\n",
    "df_vix.index = pd.to_datetime(df_vix.index)\n",
    "print(df_vix.head(10))\n",
    "print(df_vix.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5474f2e-8ac0-47b0-b250-3fdfa05f1e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIX    49\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dif_vix = df_vix.sort_index()\n",
    "print(df_vix.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0155ed8a-c5f9-41c8-8d84-acbbceea2341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-01-01', '2018-01-15', '2018-02-19', '2018-03-30',\n",
      "               '2018-05-28', '2018-07-04', '2018-09-03', '2018-11-22',\n",
      "               '2018-12-05', '2018-12-25', '2019-01-01', '2019-01-21',\n",
      "               '2019-02-18', '2019-04-19', '2019-05-27', '2019-07-04',\n",
      "               '2019-09-02', '2019-11-28', '2019-12-25', '2020-01-01',\n",
      "               '2020-01-20', '2020-02-17', '2020-04-10', '2020-05-25',\n",
      "               '2020-07-03', '2020-09-07', '2020-11-26', '2020-12-25',\n",
      "               '2021-01-01', '2021-01-18', '2021-02-15', '2021-04-02',\n",
      "               '2021-05-31', '2021-07-05', '2021-09-06', '2021-11-25',\n",
      "               '2021-12-24', '2022-01-17', '2022-02-21', '2022-04-15',\n",
      "               '2022-12-26', '2023-01-02', '2023-04-07', '2023-12-25',\n",
      "               '2024-01-01', '2024-03-29', '2024-12-25', '2025-01-01',\n",
      "               '2025-04-18'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "missing_dates =  df_vix[df_vix[\"VIX\"].isna()].index\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd7102e-c800-476b-86de-18a487a5fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vix = df_vix.dropna(subset=[\"VIX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e275a5-327f-47c5-b895-b42a5b45696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vix[\"VIX_return\"] = np.log(df_vix[\"VIX\"] / df_vix[\"VIX\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449fad97-e130-4921-ba60-af94b765cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VIX  VIX_return\n",
      "2018-01-02   9.77         NaN\n",
      "2018-01-03   9.15   -0.065563\n",
      "2018-01-04   9.22    0.007621\n",
      "2018-01-05   9.22    0.000000\n",
      "2018-01-08   9.52    0.032020\n",
      "...           ...         ...\n",
      "2025-10-24  16.37   -0.055256\n",
      "2025-10-27  15.79   -0.036074\n",
      "2025-10-28  16.42    0.039123\n",
      "2025-10-29  16.92    0.029996\n",
      "2025-10-30  16.91   -0.000591\n",
      "\n",
      "[1995 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_vix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecd92ec-dec5-4ab8-a96d-10954cb7f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIX           0\n",
      "VIX_return    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dif_vix = df_vix.sort_index()\n",
    "print(df_vix.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6697f57-47d9-4d4d-affb-651d1bbdeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vix.index = pd.to_datetime(df_vix.index)\n",
    "df_sent.index = pd.to_datetime(df_sent.index)\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3bd314-3322-4b1c-a508-fb8ff8460931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims      cpi  \\\n",
      "2018-01-01    NaN         NaN       NaN          NaN             NaN  248.859   \n",
      "2018-01-02   9.77         NaN    315.61  2695.810059             NaN      NaN   \n",
      "2018-01-03   9.15   -0.065563    315.61  2713.060059             NaN      NaN   \n",
      "2018-01-04   9.22    0.007621    317.07  2723.989990             NaN      NaN   \n",
      "2018-01-05   9.22    0.000000    318.38  2743.149902             NaN      NaN   \n",
      "2018-01-06    NaN         NaN       NaN          NaN        252000.0      NaN   \n",
      "2018-01-07    NaN         NaN       NaN          NaN             NaN      NaN   \n",
      "2018-01-08   9.52    0.032020    318.38  2747.709961             NaN      NaN   \n",
      "2018-01-09  10.08    0.057158    317.86  2751.290039             NaN      NaN   \n",
      "2018-01-10   9.82   -0.026132    316.48  2748.229980             NaN      NaN   \n",
      "2018-01-11   9.88    0.006091    315.86  2767.560059             NaN      NaN   \n",
      "2018-01-12  10.16    0.027946    314.88  2786.239990             NaN      NaN   \n",
      "2018-01-13    NaN         NaN       NaN          NaN        230000.0      NaN   \n",
      "2018-01-14    NaN         NaN       NaN          NaN             NaN      NaN   \n",
      "2018-01-15    NaN         NaN       NaN          NaN             NaN      NaN   \n",
      "2018-01-16  11.66    0.137706    315.54  2776.419922             NaN      NaN   \n",
      "2018-01-17  11.91    0.021214    315.27  2802.560059             NaN      NaN   \n",
      "2018-01-18  12.22    0.025696    315.41  2798.030029             NaN      NaN   \n",
      "2018-01-19  11.27   -0.080930    316.68  2810.300049             NaN      NaN   \n",
      "2018-01-20    NaN         NaN       NaN          NaN        231000.0      NaN   \n",
      "2018-01-21    NaN         NaN       NaN          NaN             NaN      NaN   \n",
      "2018-01-22  11.03   -0.021525    317.47  2832.969971             NaN      NaN   \n",
      "2018-01-23  11.10    0.006326    318.06  2839.129883             NaN      NaN   \n",
      "2018-01-24  11.47    0.032790    317.03  2837.540039             NaN      NaN   \n",
      "2018-01-25  11.58    0.009545    316.16  2839.250000             NaN      NaN   \n",
      "2018-01-26  11.08   -0.044138    316.66  2872.870117             NaN      NaN   \n",
      "2018-01-27    NaN         NaN       NaN          NaN        228000.0      NaN   \n",
      "2018-01-28    NaN         NaN       NaN          NaN             NaN      NaN   \n",
      "2018-01-29  13.84    0.222421    315.75  2853.530029             NaN      NaN   \n",
      "2018-01-30  14.79    0.066388    314.54  2822.429932             NaN      NaN   \n",
      "\n",
      "            term_spread  dgs10  credit_spread  \n",
      "2018-01-01          NaN    NaN           1.68  \n",
      "2018-01-02         1.02   2.46            NaN  \n",
      "2018-01-03         1.03   2.44            NaN  \n",
      "2018-01-04         1.05   2.46            NaN  \n",
      "2018-01-05         1.08   2.47            NaN  \n",
      "2018-01-06          NaN    NaN            NaN  \n",
      "2018-01-07          NaN    NaN            NaN  \n",
      "2018-01-08         1.04   2.49            NaN  \n",
      "2018-01-09         1.11   2.55            NaN  \n",
      "2018-01-10         1.13   2.55            NaN  \n",
      "2018-01-11         1.11   2.54            NaN  \n",
      "2018-01-12         1.12   2.55            NaN  \n",
      "2018-01-13          NaN    NaN            NaN  \n",
      "2018-01-14          NaN    NaN            NaN  \n",
      "2018-01-15          NaN    NaN            NaN  \n",
      "2018-01-16         1.09   2.54            NaN  \n",
      "2018-01-17         1.13   2.57            NaN  \n",
      "2018-01-18         1.17   2.62            NaN  \n",
      "2018-01-19         1.20   2.64            NaN  \n",
      "2018-01-20          NaN    NaN            NaN  \n",
      "2018-01-21          NaN    NaN            NaN  \n",
      "2018-01-22         1.22   2.66            NaN  \n",
      "2018-01-23         1.19   2.63            NaN  \n",
      "2018-01-24         1.22   2.65            NaN  \n",
      "2018-01-25         1.21   2.63            NaN  \n",
      "2018-01-26         1.25   2.66            NaN  \n",
      "2018-01-27          NaN    NaN            NaN  \n",
      "2018-01-28          NaN    NaN            NaN  \n",
      "2018-01-29         1.26   2.70            NaN  \n",
      "2018-01-30         1.29   2.73            NaN  \n",
      "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims  cpi  \\\n",
      "2025-10-01  16.29    0.000614    348.99  6711.200195             NaN  NaN   \n",
      "2025-10-02  16.63    0.020657    350.34  6715.350098             NaN  NaN   \n",
      "2025-10-03  16.65    0.001202    350.90  6715.790039             NaN  NaN   \n",
      "2025-10-04    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-05    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-06  16.37   -0.016960    350.23  6740.279785             NaN  NaN   \n",
      "2025-10-07  17.24    0.051782    349.62  6714.589844             NaN  NaN   \n",
      "2025-10-08  16.30   -0.056067    350.41  6753.720215             NaN  NaN   \n",
      "2025-10-09  16.43    0.007944    350.05  6735.109863             NaN  NaN   \n",
      "2025-10-10  21.66    0.276358    350.56  6552.509766             NaN  NaN   \n",
      "2025-10-11    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-12    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-13  19.03   -0.129451    350.56  6654.720215             NaN  NaN   \n",
      "2025-10-14  20.81    0.089417    352.64  6644.310059             NaN  NaN   \n",
      "2025-10-15  20.64   -0.008203    353.55  6671.060059             NaN  NaN   \n",
      "2025-10-16  25.31    0.203969    353.50  6629.069824             NaN  NaN   \n",
      "2025-10-17  20.78   -0.197209    353.04  6664.009766             NaN  NaN   \n",
      "2025-10-18    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-19    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-20  18.23   -0.130922    353.05  6735.129883             NaN  NaN   \n",
      "2025-10-21  17.87   -0.019945    353.69  6735.350098             NaN  NaN   \n",
      "2025-10-22  18.60    0.040038    352.76  6699.399902             NaN  NaN   \n",
      "2025-10-23  17.30   -0.072455    351.30  6738.439941             NaN  NaN   \n",
      "2025-10-24  16.37   -0.055256    350.91  6791.689941             NaN  NaN   \n",
      "2025-10-25    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-26    NaN         NaN       NaN          NaN             NaN  NaN   \n",
      "2025-10-27  15.79   -0.036074    351.19  6875.160156             NaN  NaN   \n",
      "2025-10-28  16.42    0.039123    352.30  6890.890137             NaN  NaN   \n",
      "2025-10-29  16.92    0.029996    351.29  6890.589844             NaN  NaN   \n",
      "2025-10-30  16.91   -0.000591    349.96          NaN             NaN  NaN   \n",
      "\n",
      "            term_spread  dgs10  credit_spread  \n",
      "2025-10-01         0.11   4.12           1.68  \n",
      "2025-10-02         0.08   4.10            NaN  \n",
      "2025-10-03         0.10   4.13            NaN  \n",
      "2025-10-04          NaN    NaN            NaN  \n",
      "2025-10-05          NaN    NaN            NaN  \n",
      "2025-10-06         0.16   4.18            NaN  \n",
      "2025-10-07         0.13   4.14            NaN  \n",
      "2025-10-08         0.12   4.13            NaN  \n",
      "2025-10-09         0.11   4.14            NaN  \n",
      "2025-10-10         0.03   4.05            NaN  \n",
      "2025-10-11          NaN    NaN            NaN  \n",
      "2025-10-12          NaN    NaN            NaN  \n",
      "2025-10-13          NaN    NaN            NaN  \n",
      "2025-10-14         0.01   4.03            NaN  \n",
      "2025-10-15         0.02   4.05            NaN  \n",
      "2025-10-16        -0.03   3.99            NaN  \n",
      "2025-10-17         0.02   4.02            NaN  \n",
      "2025-10-18          NaN    NaN            NaN  \n",
      "2025-10-19          NaN    NaN            NaN  \n",
      "2025-10-20         0.03   4.00            NaN  \n",
      "2025-10-21         0.02   3.98            NaN  \n",
      "2025-10-22         0.01   3.97            NaN  \n",
      "2025-10-23         0.06   4.01            NaN  \n",
      "2025-10-24         0.09   4.02            NaN  \n",
      "2025-10-25          NaN    NaN            NaN  \n",
      "2025-10-26          NaN    NaN            NaN  \n",
      "2025-10-27         0.12   4.01            NaN  \n",
      "2025-10-28         0.10   3.99            NaN  \n",
      "2025-10-29         0.15   4.08            NaN  \n",
      "2025-10-30         0.19   4.11            NaN  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2861 entries, 2018-01-01 to 2025-10-30\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   VIX             1996 non-null   float64\n",
      " 1   VIX_return      1995 non-null   float64\n",
      " 2   SGIXSENT        1968 non-null   float64\n",
      " 3   ^GSPC           1968 non-null   float64\n",
      " 4   initial_claims  403 non-null    float64\n",
      " 5   cpi             93 non-null     float64\n",
      " 6   term_spread     1958 non-null   float64\n",
      " 7   dgs10           1958 non-null   float64\n",
      " 8   credit_spread   94 non-null     float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 223.5 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "merged = pd.concat([df_vix, df_sent, sp500, df], axis=1, join=\"outer\")\n",
    "merged = merged.sort_index()\n",
    "merged = merged.loc[\"2018-01-01\" : \"2025-10-30\"]\n",
    "print(merged.head(30))\n",
    "print(merged.tail(30))\n",
    "print(merged.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c258ce0-df54-4cc2-aac5-9e39c868d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Freq: ME, Name: cpi, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "cols_ffill_others = [\"SGIXSENT\", \"cpi\", \"credit_spread\", \"term_spread\", \"dgs10\"]\n",
    "merged_full = merged.copy()\n",
    "merged_full[cols_ffill_others] = merged_full[cols_ffill_others].ffill()\n",
    "cutoff = pd.Timestamp(\"2025-09-22\")\n",
    "s = merged[\"initial_claims\"]\n",
    "s_b = s.bfill()\n",
    "s_f = s.ffill()\n",
    "initial_new = s.copy()\n",
    "mask_before = initial_new.index < cutoff\n",
    "initial_new.loc[mask_before] = s_b.loc[mask_before]\n",
    "mask_after = initial_new.index >= cutoff\n",
    "initial_new.loc[mask_after] = s_f.loc[mask_after]\n",
    "merged_full[\"initial_claims\"] = initial_new\n",
    "merged_trading = merged_full[merged_full[\"^GSPC\"].notna()].copy()\n",
    "check_cpi = merged_trading[\"cpi\"].notna().astype(int).resample(\"ME\").sum()\n",
    "print(check_cpi[check_cpi == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "029c605e-f46c-4bff-81b8-989cf141f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims      cpi  \\\n",
      "2018-01-02   9.77         NaN    315.61  2695.810059        252000.0  248.859   \n",
      "2018-01-03   9.15   -0.065563    315.61  2713.060059        252000.0  248.859   \n",
      "2018-01-04   9.22    0.007621    317.07  2723.989990        252000.0  248.859   \n",
      "2018-01-05   9.22    0.000000    318.38  2743.149902        252000.0  248.859   \n",
      "2018-01-08   9.52    0.032020    318.38  2747.709961        230000.0  248.859   \n",
      "2018-01-09  10.08    0.057158    317.86  2751.290039        230000.0  248.859   \n",
      "2018-01-10   9.82   -0.026132    316.48  2748.229980        230000.0  248.859   \n",
      "2018-01-11   9.88    0.006091    315.86  2767.560059        230000.0  248.859   \n",
      "2018-01-12  10.16    0.027946    314.88  2786.239990        230000.0  248.859   \n",
      "2018-01-16  11.66    0.137706    315.54  2776.419922        231000.0  248.859   \n",
      "\n",
      "            term_spread  dgs10  credit_spread  \n",
      "2018-01-02         1.02   2.46           1.68  \n",
      "2018-01-03         1.03   2.44           1.68  \n",
      "2018-01-04         1.05   2.46           1.68  \n",
      "2018-01-05         1.08   2.47           1.68  \n",
      "2018-01-08         1.04   2.49           1.68  \n",
      "2018-01-09         1.11   2.55           1.68  \n",
      "2018-01-10         1.13   2.55           1.68  \n",
      "2018-01-11         1.11   2.54           1.68  \n",
      "2018-01-12         1.12   2.55           1.68  \n",
      "2018-01-16         1.09   2.54           1.68  \n",
      "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims      cpi  \\\n",
      "2025-10-16  25.31    0.203969    353.50  6629.069824        218000.0  324.368   \n",
      "2025-10-17  20.78   -0.197209    353.04  6664.009766        218000.0  324.368   \n",
      "2025-10-20  18.23   -0.130922    353.05  6735.129883        218000.0  324.368   \n",
      "2025-10-21  17.87   -0.019945    353.69  6735.350098        218000.0  324.368   \n",
      "2025-10-22  18.60    0.040038    352.76  6699.399902        218000.0  324.368   \n",
      "2025-10-23  17.30   -0.072455    351.30  6738.439941        218000.0  324.368   \n",
      "2025-10-24  16.37   -0.055256    350.91  6791.689941        218000.0  324.368   \n",
      "2025-10-27  15.79   -0.036074    351.19  6875.160156        218000.0  324.368   \n",
      "2025-10-28  16.42    0.039123    352.30  6890.890137        218000.0  324.368   \n",
      "2025-10-29  16.92    0.029996    351.29  6890.589844        218000.0  324.368   \n",
      "\n",
      "            term_spread  dgs10  credit_spread  \n",
      "2025-10-16        -0.03   3.99           1.68  \n",
      "2025-10-17         0.02   4.02           1.68  \n",
      "2025-10-20         0.03   4.00           1.68  \n",
      "2025-10-21         0.02   3.98           1.68  \n",
      "2025-10-22         0.01   3.97           1.68  \n",
      "2025-10-23         0.06   4.01           1.68  \n",
      "2025-10-24         0.09   4.02           1.68  \n",
      "2025-10-27         0.12   4.01           1.68  \n",
      "2025-10-28         0.10   3.99           1.68  \n",
      "2025-10-29         0.15   4.08           1.68  \n"
     ]
    }
   ],
   "source": [
    "print(merged_trading.head(10))\n",
    "print(merged_trading.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c731b1d2-9713-4055-84b9-45ddddedd7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VIX               0\n",
       "VIX_return        1\n",
       "SGIXSENT          0\n",
       "^GSPC             0\n",
       "initial_claims    0\n",
       "cpi               0\n",
       "term_spread       0\n",
       "dgs10             0\n",
       "credit_spread     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_trading.isna().any().any()\n",
    "merged_trading.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9a50ad-aea8-4244-bb33-17c159718a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_trading[\"VIX_return\"] = merged_trading[\"VIX_return\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f030c0a-e51e-406a-a52a-12cac565a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_trading.to_csv(\"merged_trading.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91bbb0-573c-4109-b6c8-dee3df0f9a20",
   "metadata": {},
   "source": [
    "I applied **front-fill** for `\"SGIXSENT\"`, `\"cpi\"`, and `\"credit_spread\"`.  \n",
    "\n",
    "For `\"Initial_Claims\"`, I used **back-fill** before *2025-09-22*, since the data is published every Saturday (this is the most recent release available from FRED).  \n",
    "After *2025-09-22*, I switched to **front-fill** for `\"Initial_Claims\"`.  \n",
    "\n",
    "In addition, `\"VIX_return\"`  had no value on *2018-01-02* due to the calculation method, so I assigned it a value of **0** in the CSV file.\n",
    "\n",
    "Also, I removed daily observations where the **U.S. stock market was closed**, keeping only the dates when the S&P 500 (`^GSPC`) had available data.\n",
    "\n",
    "2025-10-13 (Columbus Day) :The stock market remains open, but the bond market is closed. So I frontfill **term_spread**, **dgs10** for this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80449785-6b8d-4352-9ce7-97b9df6eedc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 days    1538\n",
       "3 days     355\n",
       "4 days      53\n",
       "2 days      21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_trading.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92ad880c-898a-47f9-9e8c-56aec688debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1538 + 355 +53 + 21 # Confirm trading Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34ad17b-1135-4c18-90f4-addace5eec2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIX</th>\n",
       "      <th>VIX_return</th>\n",
       "      <th>SGIXSENT</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>initial_claims</th>\n",
       "      <th>cpi</th>\n",
       "      <th>term_spread</th>\n",
       "      <th>dgs10</th>\n",
       "      <th>credit_spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>9.77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>315.61</td>\n",
       "      <td>2695.810059</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>9.15</td>\n",
       "      <td>-0.065563</td>\n",
       "      <td>315.61</td>\n",
       "      <td>2713.060059</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>9.22</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>317.07</td>\n",
       "      <td>2723.989990</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>9.22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.38</td>\n",
       "      <td>2743.149902</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>9.52</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>318.38</td>\n",
       "      <td>2747.709961</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-23</th>\n",
       "      <td>17.30</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>351.30</td>\n",
       "      <td>6738.439941</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-24</th>\n",
       "      <td>16.37</td>\n",
       "      <td>-0.055256</td>\n",
       "      <td>350.91</td>\n",
       "      <td>6791.689941</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-27</th>\n",
       "      <td>15.79</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>351.19</td>\n",
       "      <td>6875.160156</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-28</th>\n",
       "      <td>16.42</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>352.30</td>\n",
       "      <td>6890.890137</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-29</th>\n",
       "      <td>16.92</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>351.29</td>\n",
       "      <td>6890.589844</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1968 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims      cpi  \\\n",
       "2018-01-02   9.77    0.000000    315.61  2695.810059        252000.0  248.859   \n",
       "2018-01-03   9.15   -0.065563    315.61  2713.060059        252000.0  248.859   \n",
       "2018-01-04   9.22    0.007621    317.07  2723.989990        252000.0  248.859   \n",
       "2018-01-05   9.22    0.000000    318.38  2743.149902        252000.0  248.859   \n",
       "2018-01-08   9.52    0.032020    318.38  2747.709961        230000.0  248.859   \n",
       "...           ...         ...       ...          ...             ...      ...   \n",
       "2025-10-23  17.30   -0.072455    351.30  6738.439941        218000.0  324.368   \n",
       "2025-10-24  16.37   -0.055256    350.91  6791.689941        218000.0  324.368   \n",
       "2025-10-27  15.79   -0.036074    351.19  6875.160156        218000.0  324.368   \n",
       "2025-10-28  16.42    0.039123    352.30  6890.890137        218000.0  324.368   \n",
       "2025-10-29  16.92    0.029996    351.29  6890.589844        218000.0  324.368   \n",
       "\n",
       "            term_spread  dgs10  credit_spread  \n",
       "2018-01-02         1.02   2.46           1.68  \n",
       "2018-01-03         1.03   2.44           1.68  \n",
       "2018-01-04         1.05   2.46           1.68  \n",
       "2018-01-05         1.08   2.47           1.68  \n",
       "2018-01-08         1.04   2.49           1.68  \n",
       "...                 ...    ...            ...  \n",
       "2025-10-23         0.06   4.01           1.68  \n",
       "2025-10-24         0.09   4.02           1.68  \n",
       "2025-10-27         0.12   4.01           1.68  \n",
       "2025-10-28         0.10   3.99           1.68  \n",
       "2025-10-29         0.15   4.08           1.68  \n",
       "\n",
       "[1968 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff3759d-b3b6-4abe-8a24-e04559803241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization for Sentiment (for XGB using)\n",
    "\n",
    "merged_trading[\"SGIXSENT_z\"] = (\n",
    "    (merged_trading[\"SGIXSENT\"] - merged_trading[\"SGIXSENT\"].mean()) /\n",
    "     merged_trading[\"SGIXSENT\"].std()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a07b8-37e5-41d3-bba0-5b92843f56e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ada26440-deb6-47fc-8462-52fdd7512e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import os\n",
    "\n",
    "# set seed\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b396925-3b2f-4fa8-8b9c-a601f06ef319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIX returns scale is too small, may hard to converge, but not necessary. \n",
    "# Our y scale = 0.0066, only 0.66% changes, but GARCH default scale is 1-1000. \n",
    "# It suggests x10 (returns x10), but I use raw data first.\n",
    "\n",
    "#change first date of VIX returns to NA\n",
    "merged_trading.loc[\"2018-01-02\", \"VIX_return\"] = np.nan\n",
    "returns = merged_trading[\"VIX_return\"].dropna()\n",
    "\n",
    "garch = arch_model(returns, vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n",
    "res = garch.fit(disp=\"off\")\n",
    "\n",
    "merged_trading[\"garch_resid\"] = res.resid / res.conditional_volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3306368a-9ded-48f5-b232-8971159e4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 Lag Features from GARCH residuals\n",
    "for k in range(1, 6):\n",
    "    merged_trading[f\"resid_lag{k}\"] = merged_trading[\"garch_resid\"].shift(k)\n",
    "\n",
    "# Drop first few rows with NaN lags\n",
    "data = merged_trading.dropna().copy()\n",
    "\n",
    "# Feature Set\n",
    "feature_cols = [\"resid_lag1\",\"resid_lag2\",\"resid_lag3\",\"resid_lag4\",\"resid_lag5\",\n",
    "                \"SGIXSENT\", \"^GSPC\", \"initial_claims\", \"cpi\", \"term_spread\", \"dgs10\", \"credit_spread\"]\n",
    "target_col = \"VIX_return\"\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d0d8547-6baa-46f2-897a-29d5fd2fd004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIX</th>\n",
       "      <th>VIX_return</th>\n",
       "      <th>SGIXSENT</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>initial_claims</th>\n",
       "      <th>cpi</th>\n",
       "      <th>term_spread</th>\n",
       "      <th>dgs10</th>\n",
       "      <th>credit_spread</th>\n",
       "      <th>SGIXSENT_z</th>\n",
       "      <th>garch_resid</th>\n",
       "      <th>resid_lag1</th>\n",
       "      <th>resid_lag2</th>\n",
       "      <th>resid_lag3</th>\n",
       "      <th>resid_lag4</th>\n",
       "      <th>resid_lag5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>9.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315.61</td>\n",
       "      <td>2695.810059</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.02</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.504842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>9.15</td>\n",
       "      <td>-0.065563</td>\n",
       "      <td>315.61</td>\n",
       "      <td>2713.060059</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.44</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.504842</td>\n",
       "      <td>-0.530604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>9.22</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>317.07</td>\n",
       "      <td>2723.989990</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.441465</td>\n",
       "      <td>0.086161</td>\n",
       "      <td>-0.530604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>9.22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.38</td>\n",
       "      <td>2743.149902</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.384598</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.086161</td>\n",
       "      <td>-0.530604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>9.52</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>318.38</td>\n",
       "      <td>2747.709961</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>248.859</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.68</td>\n",
       "      <td>-1.384598</td>\n",
       "      <td>0.448157</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.086161</td>\n",
       "      <td>-0.530604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-23</th>\n",
       "      <td>17.30</td>\n",
       "      <td>-0.072455</td>\n",
       "      <td>351.30</td>\n",
       "      <td>6738.439941</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.044437</td>\n",
       "      <td>-0.802429</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>-0.144422</td>\n",
       "      <td>-0.914172</td>\n",
       "      <td>-1.516666</td>\n",
       "      <td>2.173576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-24</th>\n",
       "      <td>16.37</td>\n",
       "      <td>-0.055256</td>\n",
       "      <td>350.91</td>\n",
       "      <td>6791.689941</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.02</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.027507</td>\n",
       "      <td>-0.643402</td>\n",
       "      <td>-0.802429</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>-0.144422</td>\n",
       "      <td>-0.914172</td>\n",
       "      <td>-1.516666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-27</th>\n",
       "      <td>15.79</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>351.19</td>\n",
       "      <td>6875.160156</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.039662</td>\n",
       "      <td>-0.447449</td>\n",
       "      <td>-0.643402</td>\n",
       "      <td>-0.802429</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>-0.144422</td>\n",
       "      <td>-0.914172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-28</th>\n",
       "      <td>16.42</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>352.30</td>\n",
       "      <td>6890.890137</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>0.566179</td>\n",
       "      <td>-0.447449</td>\n",
       "      <td>-0.643402</td>\n",
       "      <td>-0.802429</td>\n",
       "      <td>0.396691</td>\n",
       "      <td>-0.144422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-29</th>\n",
       "      <td>16.92</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>351.29</td>\n",
       "      <td>6890.589844</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>324.368</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.458367</td>\n",
       "      <td>0.566179</td>\n",
       "      <td>-0.447449</td>\n",
       "      <td>-0.643402</td>\n",
       "      <td>-0.802429</td>\n",
       "      <td>0.396691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1968 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              VIX  VIX_return  SGIXSENT        ^GSPC  initial_claims      cpi  \\\n",
       "2018-01-02   9.77         NaN    315.61  2695.810059        252000.0  248.859   \n",
       "2018-01-03   9.15   -0.065563    315.61  2713.060059        252000.0  248.859   \n",
       "2018-01-04   9.22    0.007621    317.07  2723.989990        252000.0  248.859   \n",
       "2018-01-05   9.22    0.000000    318.38  2743.149902        252000.0  248.859   \n",
       "2018-01-08   9.52    0.032020    318.38  2747.709961        230000.0  248.859   \n",
       "...           ...         ...       ...          ...             ...      ...   \n",
       "2025-10-23  17.30   -0.072455    351.30  6738.439941        218000.0  324.368   \n",
       "2025-10-24  16.37   -0.055256    350.91  6791.689941        218000.0  324.368   \n",
       "2025-10-27  15.79   -0.036074    351.19  6875.160156        218000.0  324.368   \n",
       "2025-10-28  16.42    0.039123    352.30  6890.890137        218000.0  324.368   \n",
       "2025-10-29  16.92    0.029996    351.29  6890.589844        218000.0  324.368   \n",
       "\n",
       "            term_spread  dgs10  credit_spread  SGIXSENT_z  garch_resid  \\\n",
       "2018-01-02         1.02   2.46           1.68   -1.504842          NaN   \n",
       "2018-01-03         1.03   2.44           1.68   -1.504842    -0.530604   \n",
       "2018-01-04         1.05   2.46           1.68   -1.441465     0.086161   \n",
       "2018-01-05         1.08   2.47           1.68   -1.384598     0.014649   \n",
       "2018-01-08         1.04   2.49           1.68   -1.384598     0.448157   \n",
       "...                 ...    ...            ...         ...          ...   \n",
       "2025-10-23         0.06   4.01           1.68    0.044437    -0.802429   \n",
       "2025-10-24         0.09   4.02           1.68    0.027507    -0.643402   \n",
       "2025-10-27         0.12   4.01           1.68    0.039662    -0.447449   \n",
       "2025-10-28         0.10   3.99           1.68    0.087846     0.566179   \n",
       "2025-10-29         0.15   4.08           1.68    0.044002     0.458367   \n",
       "\n",
       "            resid_lag1  resid_lag2  resid_lag3  resid_lag4  resid_lag5  \n",
       "2018-01-02         NaN         NaN         NaN         NaN         NaN  \n",
       "2018-01-03         NaN         NaN         NaN         NaN         NaN  \n",
       "2018-01-04   -0.530604         NaN         NaN         NaN         NaN  \n",
       "2018-01-05    0.086161   -0.530604         NaN         NaN         NaN  \n",
       "2018-01-08    0.014649    0.086161   -0.530604         NaN         NaN  \n",
       "...                ...         ...         ...         ...         ...  \n",
       "2025-10-23    0.396691   -0.144422   -0.914172   -1.516666    2.173576  \n",
       "2025-10-24   -0.802429    0.396691   -0.144422   -0.914172   -1.516666  \n",
       "2025-10-27   -0.643402   -0.802429    0.396691   -0.144422   -0.914172  \n",
       "2025-10-28   -0.447449   -0.643402   -0.802429    0.396691   -0.144422  \n",
       "2025-10-29    0.566179   -0.447449   -0.643402   -0.802429    0.396691  \n",
       "\n",
       "[1968 rows x 16 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ce1db-f79f-4ddf-9e73-4c2752d41f34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. XGBoost Traning with rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aa3c2d9-4e95-4bba-a9f9-602320187b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.8)  # 80% training period\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "for i in range(train_size, len(data)):\n",
    "    X_train = X.iloc[:i]\n",
    "    y_train = y.iloc[:i]\n",
    "    X_test = X.iloc[i:i+1]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,              \n",
    "        tree_method=\"exact\"  \n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)[0]\n",
    "\n",
    "    predictions.append(pred)\n",
    "    true_values.append(y.iloc[i])\n",
    "\n",
    "# XGBoost Hybrid Model Forecast Series\n",
    "hybrid_pred = np.array(predictions)\n",
    "true_values = np.array(true_values)\n",
    "\n",
    "#To avoid look-ahead bias, we adopt a recursive rolling forecasting framework.\n",
    "#At each time step t, the XGBoost model is re-trained only on information available prior to t, and generates a one-step-ahead forecast for VIX returns.\n",
    "#This ensures that the hybrid model is evaluated under realistic real-time forecasting conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba63eff-f72a-4d3a-b7a0-d526999271e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Baseline GARCH Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711c65c7-93a1-4b7f-adf2-7e31f7610ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. Baseline GARCH Forecasting (Aligned with Hybrid)\n",
    "# ==============================\n",
    "\n",
    "# 使用 hybrid 已經 dropna 後的資料\n",
    "returns_aligned = data[target_col]      # 完全與 hybrid 一致的 index\n",
    "train_size = int(len(returns_aligned) * 0.8)\n",
    "\n",
    "def garch_expanding_forecast_aligned(returns, start_idx):\n",
    "    preds = [np.nan] * start_idx\n",
    "    for i in range(start_idx, len(returns)):\n",
    "        train_data = returns.iloc[:i]   # expanding window\n",
    "        model = arch_model(train_data, vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n",
    "        res = model.fit(disp=\"off\")\n",
    "        f = res.forecast(horizon=1).mean.iloc[-1, 0]   # return mean forecast\n",
    "        preds.append(f)\n",
    "    return pd.Series(preds, index=returns.index, name=\"garch_pred\")\n",
    "\n",
    "# 使用與 hybrid 完全相同的 index 和樣本\n",
    "garch_series = garch_expanding_forecast_aligned(returns_aligned, train_size)\n",
    "\n",
    "# OOS slicing 完全相同\n",
    "garch_pred = garch_series.iloc[train_size:].values\n",
    "hybrid_pred = hybrid_pred\n",
    "true_vals = true_values\n",
    "\n",
    "# 對齊長度（若最後幾筆不同）\n",
    "min_len = min(len(garch_pred), len(hybrid_pred), len(true_vals))\n",
    "garch_pred = garch_pred[:min_len]\n",
    "hybrid_pred = hybrid_pred[:min_len]\n",
    "true_vals = true_vals[:min_len]\n",
    "\n",
    "# 既然我已經讓 Hybrid 和 GARCH 用相同 index，為什麼 STILL 有可能最後幾筆長度不同？ → 兩種模型的 rolling forecasting 發生在不同的迴圈裡，每一筆預測能否成功產生取決於模型是否能收斂（fit 得動）。\n",
    "# GARCH 並不保證每一個 expanding window 都能成功 fit。GARCH 在某些 window 可能因為：過度平滑、波動太低, 急遽跳動（VIX spike）, convergence problem（常見）, “Hessian modified” 但仍無法收斂 → 就會產生 NaN 或預測失敗。\n",
    "# XGB 在相同 window 不會失敗 → 長度因此不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082252c0-43ec-43c6-bbf9-4f9ede0d9550",
   "metadata": {},
   "source": [
    "回答你的問題: 用garch conditional mean 當baseline可以嗎?\n",
    "(其實真實的vix returns也是mean=0)\n",
    "\n",
    "✅ 結論先說：\n",
    "Yes，GARCH 的 forecast（conditional mean return）幾乎 ≈ 0，但你仍然可以合理地把它拿來和 XGBoost 比較。\n",
    "\n",
    "理由如下（很重要）：\n",
    "\n",
    "✅ 1. GARCH 的 conditional mean ≠ “永遠 = 0”，只是「接近 0」\n",
    "\n",
    "對於 VIX returns 這類：\n",
    "\n",
    "高噪音（noise-dominated）\n",
    "\n",
    "幾乎沒有可預測的 drift\n",
    "\n",
    "強烈 mean reversion（但不是平均 return），是 volatility mean reversion\n",
    "\n",
    "→ GARCH 在 mean equation 預測的 μₜ ≈ 0 是正常且文獻共通的發現。\n",
    "\n",
    "但並不是「永遠等於 0」：\n",
    "→ 有波動\n",
    "→ 不全為 0\n",
    "→ 是 real forecasts\n",
    "→ 可比較\n",
    "\n",
    "✅ 2. 那為什麼還可以跟 XGBoost 做 RMSE 比較？\n",
    "因為兩者 predict 的 target 完全相同：\n",
    "\n",
    "模型\t預測的東西\n",
    "GARCH\tone-step-ahead conditional mean \n",
    "XGB Hybrid\tone-step-ahead return \n",
    "\n",
    "→ 兩者都是 deterministic forecasts\n",
    "→ 兩者都是「下一期 return 的點估計」\n",
    "\n",
    "所以 RMSE、MAE 完全可以比較。\n",
    "\n",
    "❗ 但會看到一個自然結果：\n",
    "在 1-day horizon：GARCH 幾乎一定比 ML 好\n",
    "\n",
    "因為：\n",
    "\n",
    "VIX daily returns 幾乎全是噪音（unpredictable）\n",
    "\n",
    "ML 反而容易 overfit noise → degrade performance\n",
    "\n",
    "GARCH 的 mean ≈ 0 → 這常常就是最好的「no-change forecast」\n",
    "\n",
    "📌 許多 VIX forecasting 的文獻都發現：\n",
    "短期（1～2天）直接預測 return 幾乎沒有任何模型能打敗 zero-mean benchmark。\n",
    "\n",
    "✔ 這樣就很合理為什麼你得到：\n",
    "\n",
    "GARCH RMSE ≈ 0.090\n",
    "\n",
    "Hybrid RMSE ≈ 0.098（更差）\n",
    "\n",
    "→ 這不是你做錯，\n",
    "→ 這是現實、是VIX 資料的本質、也是文獻共通現象。\n",
    "\n",
    "✅ 3. “Return ≈ 0 baseline” 本身就是一個很強的 baseline\n",
    "\n",
    "這個 baseline 叫做：\n",
    "\n",
    "Random Walk with Zero Drift\n",
    "𝑟\n",
    "^\n",
    "𝑡\n",
    "+\n",
    "1\n",
    "=\n",
    "0\n",
    "r\n",
    "^\n",
    "t+1\n",
    "\t​\n",
    "\n",
    "=0\n",
    "\n",
    "它在金融時間序列裡非常強大，因為：\n",
    "\n",
    "大部分資產的短期回報率 不可預測\n",
    "\n",
    "噪音比 signal 大太多\n",
    "\n",
    "任何試圖預測 Return 本身的模型都容易 overfit\n",
    "\n",
    "因此：\n",
    "\n",
    "**GARCH(1,1) mean forecast ≈ 0\n",
    "\n",
    "其實就是一個非常合理、文獻認可的 baseline！**\n",
    "\n",
    "⭐ 4. 那 Hybrid 是否會永遠輸？怎麼讓 Hybrid 贏？\n",
    "\n",
    "No，它在以下兩種情況會贏：\n",
    "\n",
    "（A）Mid-to-long horizons（5-day, 22-day cumulative returns）\n",
    "\n",
    "因為：\n",
    "\n",
    "噪音在累積時會被平均掉\n",
    "\n",
    "macro + sentiment 在長期才影響 VIX returns\n",
    "\n",
    "ML 得以捕捉 longer-term nonlinear structure\n",
    "\n",
    "你已經看到 5-day / 22-day 的 RMSE 差距縮小\n",
    "表示 Hybrid 逐漸接近甚至可打敗 GARCH。\n",
    "\n",
    "（B）如果你改成預測 GARCH residual（而不是 raw returns）\n",
    "\n",
    "Hybrid 會變強很多。\n",
    "\n",
    "因為 residual 是：\n",
    "\n",
    "已經去除了 volatility clustering\n",
    "\n",
    "更乾淨\n",
    "\n",
    "更可預測\n",
    "\n",
    "→ Hybrid 專門 “補 GARCH 沒捕捉到的非線性”\n",
    "\n",
    "這才是許多 GARCH-ML paper 的本體。\n",
    "\n",
    "🚨 最重要提醒：你現在做的是：預測 VIX daily return → 這本來就難預測。\n",
    "\n",
    "要 Hybrid 贏，你必須：\n",
    "\n",
    "✔ 改成 multi-horizon\n",
    "\n",
    "（你有做，正在改善）\n",
    "\n",
    "✔ 或改成預測 residual\n",
    "\n",
    "（你還沒做，可以讓 XGB 勝出）\n",
    "\n",
    "🔥 5. 是否要把 GARCH mean forecast vs XGB forecast 拿來比？ YES！這在學術上完全正確。\n",
    "\n",
    "因為：\n",
    "\n",
    "一樣的 target\n",
    "\n",
    "一樣的 horizon\n",
    "\n",
    "一樣是 deterministic point forecast\n",
    "\n",
    "一樣用 RMSE / MAE / direction accuracy 評估\n",
    "\n",
    "→ 學術上 100% 合法、合理、標準\n",
    "\n",
    "⭐ 最後簡短答案：\n",
    "✔ GARCH mean ≈ 0 很正常，不代表不能比較\n",
    "✔ Hybrid 不可能在短期 return 預測上贏 GARCH\n",
    "✔ Hybrid 有可能在 5-day / 22-day 打敗 GARCH\n",
    "✔ Hybrid 若改成預測 GARCH residual → 最強組合\n",
    "✔ 你目前的比較方法完全正確（沒有錯）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d10c78-d38f-4882-8f63-9a3348942757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Models evaluation by RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae673a17-a7b8-4045-a90e-e2087cc99e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays of equal length\n",
    "garch_pred = garch_series.iloc[train_size:].values\n",
    "hybrid_pred = hybrid_pred\n",
    "true_vals = true_values\n",
    "\n",
    "min_len = min(len(garch_pred), len(hybrid_pred), len(true_vals))\n",
    "garch_pred = garch_pred[:min_len]\n",
    "hybrid_pred = hybrid_pred[:min_len]\n",
    "true_vals = true_vals[:min_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1383aff3-767c-477a-931e-53d356af9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - GARCH: 0.09040210731407829\n",
      "RMSE - Hybrid: 0.09845520035524992\n",
      "MAE  - GARCH: 0.05861192510922363\n",
      "MAE  - Hybrid: 0.06272420304422699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#Compute RMSE and MAE\n",
    "rmse_garch = np.sqrt(mean_squared_error(true_vals, garch_pred))\n",
    "rmse_hybrid = np.sqrt(mean_squared_error(true_vals, hybrid_pred))\n",
    "mae_garch = mean_absolute_error(true_vals, garch_pred)\n",
    "mae_hybrid = mean_absolute_error(true_vals, hybrid_pred)\n",
    "\n",
    "print(\"RMSE - GARCH:\", rmse_garch)\n",
    "print(\"RMSE - Hybrid:\", rmse_hybrid)\n",
    "print(\"MAE  - GARCH:\", mae_garch)\n",
    "print(\"MAE  - Hybrid:\", mae_hybrid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d73b6-fb51-42a4-ac64-4903c9e209d9",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- GARCH-only models inherently good at forecasting short-term VIX volatility (especially one-day ahead) because VIX exhibits strong volatility clustering and mean reversion—precisely the assumptions underlying GARCH models.\n",
    "\n",
    "- The Hybrid model incorporates excessive noise characteristics (macro + sentiment daily alignment), which dilutes the signal.\n",
    "\n",
    "- XGBoost is prone to overfitting minor real-time noise fluctuations → leading to poor long-term performance.\n",
    "\n",
    "- our current approach is: forecasting a 1-day ahead short horizon → At this horizon, GARCH inherently holds an advantage.\n",
    "\n",
    "- Therefore, results are not only reasonable but also an “expected outcome.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "911d5f03-8ddd-4ee6-86f0-21fe727cbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hybrid model does NOT outperform GARCH. Consider feature tuning.\n"
     ]
    }
   ],
   "source": [
    "# Which one performs better?\n",
    "if rmse_hybrid < rmse_garch:\n",
    "    print(\"\\n Hybrid model improves forecast accuracy.\")\n",
    "else:\n",
    "    print(\"\\n Hybrid model does NOT outperform GARCH. Consider feature tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58407f08-cca1-47e7-9dbd-6232dc2d2738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction Accuracy - GARCH: 0.5597964376590331\n",
      "Direction Accuracy - Hybrid: 0.48346055979643765\n"
     ]
    }
   ],
   "source": [
    "#Direction accuracy\n",
    "direction_garch = np.mean(np.sign(garch_pred) == np.sign(true_vals))\n",
    "direction_hybrid = np.mean(np.sign(hybrid_pred) == np.sign(true_vals))\n",
    "\n",
    "print(\"Direction Accuracy - GARCH:\", direction_garch)\n",
    "print(\"Direction Accuracy - Hybrid:\", direction_hybrid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92397d-5615-4eb8-846b-2b70b7a7a4bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Direction Accuracy measures whether the model's predicted direction is correct.\n",
    "\n",
    "Direction Accuracy=P(sign(pred)=sign(actual))\n",
    "\n",
    "- if  > 0.5\tBetter than random (market predictability exists))\n",
    "\n",
    "- if  = 0.5\tLike flipping a coin, no predictive capability\n",
    "\n",
    "- if  < 0.5\tReverse signals may exhibit exploitable patterns\n",
    "\n",
    "Our hybrid model performs close to random guessing=0.5? \n",
    "\n",
    "##### Results Interpretation:\n",
    "1. GARCH captures short-term volatility clustering → strong short-term predictive power.\n",
    "2. Macro + sentiment factors operate at slower frequencies → hybrid model doesn’t help for 1-day horizon.\n",
    "3. This suggests hybrid models may only outperform GARCH at longer horizons (e.g., 5-day / 22-day forecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22739f-d740-4499-9342-15969b3b9978",
   "metadata": {},
   "source": [
    "# We should consider what should be our next step:\n",
    "1. multi-horizon forecasting: using 5-day(1-week), 22-day(1-month) ahead RMSE + Direction Accuracy (we use 1-day ahead beforehand)\n",
    "2. XGB Hyperparameter tuning \n",
    "3. including more features?\n",
    "\n",
    "Those methods may improve the XGB forcasting, but due to the DS class criteria, we should definately keep the above (not ideal) results and explain it. Then, move on to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089353c-4198-42f5-ab75-a3beaf50c19c",
   "metadata": {},
   "source": [
    "# 6. Multi-Horizon Forecasting (1-day, 1-week, 1-month, 3-month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb40b209-ecd2-4ec8-ade1-3090162cfb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Horizon = 1 days ahead ----\n",
      "\n",
      "---- Horizon = 5 days ahead ----\n",
      "\n",
      "---- Horizon = 22 days ahead ----\n",
      "\n",
      "---- Horizon = 66 days ahead ----\n",
      "\n",
      "\n",
      "====== Multi-Horizon Comparison (Aligned Data) ======\n",
      "    RMSE_GARCH  RMSE_HYBRID  MAE_GARCH  MAE_HYBRID  DIR_GARCH  DIR_HYBRID\n",
      "1     0.090399     0.101633   0.058608    0.066116   0.559796    0.460560\n",
      "5     0.090480     0.096608   0.058617    0.064103   0.558673    0.482143\n",
      "22    0.090532     0.100377   0.058507    0.065766   0.559278    0.463918\n",
      "66    0.091037     0.100059   0.058686    0.065745   0.557895    0.468421\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 6. Multi-Horizon Forecasting (Aligned GARCH)\n",
    "# ==============================\n",
    "\n",
    "horizons = [1, 5, 22, 66]  # 1-day, 1-week, 1-month, 3-month\n",
    "results = {}\n",
    "\n",
    "def garch_expanding_forecast_aligned(returns, start_idx):\n",
    "    preds = [np.nan] * start_idx\n",
    "    for i in range(start_idx, len(returns)):\n",
    "        train_data = returns.iloc[:i]\n",
    "        model = arch_model(train_data, vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n",
    "        res = model.fit(disp=\"off\")\n",
    "        f = res.forecast(horizon=1).mean.iloc[-1, 0]   # mean forecast\n",
    "        preds.append(f)\n",
    "    return pd.Series(preds, index=returns.index, name=\"garch_pred\")\n",
    "\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\n---- Horizon = {h} days ahead ----\")\n",
    "\n",
    "    # Horizon target (aligned)\n",
    "    y_h = data[target_col].shift(-h).dropna()\n",
    "\n",
    "    # Align features to target\n",
    "    X_h = X.loc[y_h.index]\n",
    "\n",
    "    # Rolling window split\n",
    "    train_size_h = int(len(X_h) * 0.8)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Hybrid (XGB) Forecast\n",
    "    # ------------------------------\n",
    "    hybrid_preds_h = []\n",
    "    true_vals_h = []\n",
    "\n",
    "    for i in range(train_size_h, len(X_h)):\n",
    "        X_train = X_h.iloc[:i]\n",
    "        y_train = y_h.iloc[:i]\n",
    "        X_test = X_h.iloc[i:i+1]\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=SEED,\n",
    "            n_jobs=1,\n",
    "            tree_method=\"exact\"\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)[0]\n",
    "\n",
    "        hybrid_preds_h.append(pred)\n",
    "        true_vals_h.append(y_h.iloc[i])\n",
    "\n",
    "    hybrid_preds_h = np.array(hybrid_preds_h)\n",
    "    true_vals_h = np.array(true_vals_h)\n",
    "\n",
    "    # ------------------------------\n",
    "    # GARCH (aligned)\n",
    "    # ------------------------------\n",
    "    garch_series_h = garch_expanding_forecast_aligned(y_h, train_size_h)\n",
    "    garch_preds_h = garch_series_h.iloc[train_size_h:].values\n",
    "\n",
    "    # ------------------------------\n",
    "    # Evaluation\n",
    "    # ------------------------------\n",
    "    min_len = min(len(garch_preds_h), len(hybrid_preds_h), len(true_vals_h))\n",
    "    garch_preds_h = garch_preds_h[:min_len]\n",
    "    hybrid_preds_h = hybrid_preds_h[:min_len]\n",
    "    true_vals_h = true_vals_h[:min_len]\n",
    "\n",
    "    rmse_g = np.sqrt(mean_squared_error(true_vals_h, garch_preds_h))\n",
    "    rmse_h = np.sqrt(mean_squared_error(true_vals_h, hybrid_preds_h))\n",
    "    mae_g = mean_absolute_error(true_vals_h, garch_preds_h)\n",
    "    mae_h = mean_absolute_error(true_vals_h, hybrid_preds_h)\n",
    "    dir_g = np.mean(np.sign(garch_preds_h) == np.sign(true_vals_h))\n",
    "    dir_h = np.mean(np.sign(hybrid_preds_h) == np.sign(true_vals_h))\n",
    "\n",
    "    results[h] = [rmse_g, rmse_h, mae_g, mae_h, dir_g, dir_h]\n",
    "\n",
    "\n",
    "# Convert results to comparison dataframe\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    index=[\"RMSE_GARCH\", \"RMSE_HYBRID\", \"MAE_GARCH\", \"MAE_HYBRID\", \"DIR_GARCH\", \"DIR_HYBRID\"]\n",
    ").T\n",
    "\n",
    "print(\"\\n\\n====== Multi-Horizon Comparison (Aligned Data) ======\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae401b8-fc5b-4b41-9897-d4ecc55c59f1",
   "metadata": {},
   "source": [
    "### Multi-Horizon Forecasting Results (1-day, 5-day, 22-day)\n",
    "\n",
    "- Extended the baseline 1-day-ahead forecasting to multiple horizons (1, 5, and 22 days ahead) to examine how predictive performance changes over different time scales.  \n",
    "\n",
    "- Results show that the GARCH model consistently outperforms the Hybrid XGBoost model at short horizons, reflecting volatility clustering in daily returns.  \n",
    "\n",
    "- However, the Hybrid model slightly narrows the performance gap at longer horizons (5-day and 22-day), suggesting that macro and sentiment variables may provide incremental information over medium-term horizons.  \n",
    "\n",
    "- These findings are consistent with financial theory: short-term returns are dominated by noise and volatility effects, while longer-term dynamics are more influenced by fundamentals and sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806a1d3-8622-4911-b620-cf896bd022e9",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5ff5a98-ee10-4867-8a6f-8e900c9fa8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Starting Hyperparameter Tuning (based on 1-day horizon) ======\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "\n",
      "Best parameters found:\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "\n",
      "\n",
      "====== Evaluating Tuned Model (1-day horizon) ======\n",
      "RMSE (Tuned Hybrid): 0.092344\n",
      "MAE  (Tuned Hybrid): 0.059767\n",
      "DIR  (Tuned Hybrid): 0.478372\n",
      "\n",
      "\n",
      "====== Updated Results Summary ======\n",
      "                 RMSE_GARCH  RMSE_HYBRID  MAE_GARCH  MAE_HYBRID  DIR_GARCH  \\\n",
      "1                  0.090399     0.101633   0.058608    0.066116   0.559796   \n",
      "5                  0.090480     0.096608   0.058617    0.064103   0.558673   \n",
      "22                 0.090532     0.100377   0.058507    0.065766   0.559278   \n",
      "66                 0.091037     0.100059   0.058686    0.065745   0.557895   \n",
      "Tuned_1d_Hybrid         NaN     0.092344        NaN    0.059767        NaN   \n",
      "\n",
      "                 DIR_HYBRID  \n",
      "1                  0.460560  \n",
      "5                  0.482143  \n",
      "22                 0.463918  \n",
      "66                 0.468421  \n",
      "Tuned_1d_Hybrid    0.478372  \n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 7. Hyperparameter Tuning for XGBoost (Post-analysis)\n",
    "# ==============================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print(\"\\n\\n====== Starting Hyperparameter Tuning (based on 1-day horizon) ======\")\n",
    "\n",
    "# 1. Use 1-day horizon data for tuning\n",
    "h = 1\n",
    "y_h = data[target_col].shift(-h).dropna()\n",
    "X_h = X.loc[y_h.index]\n",
    "\n",
    "train_size_h = int(len(X_h) * 0.8)\n",
    "X_train = X_h.iloc[:train_size_h]\n",
    "y_train = y_h.iloc[:train_size_h]\n",
    "\n",
    "# 2. Define parameter search grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# 3. Initialize model\n",
    "xgb_base = XGBRegressor(objective='reg:squarederror', random_state=SEED)\n",
    "\n",
    "# 4. Run grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_base, \n",
    "    param_grid, \n",
    "    scoring=\"neg_mean_squared_error\", \n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(best_params)\n",
    "\n",
    "# 5. Evaluate tuned model using the same rolling scheme (for consistency)\n",
    "print(\"\\n\\n====== Evaluating Tuned Model (1-day horizon) ======\")\n",
    "hybrid_preds_tuned = []\n",
    "true_vals_tuned = []\n",
    "\n",
    "for i in range(train_size_h, len(X_h)):\n",
    "    X_train = X_h.iloc[:i]\n",
    "    y_train = y_h.iloc[:i]\n",
    "    X_test = X_h.iloc[i:i+1]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"exact\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)[0]\n",
    "    hybrid_preds_tuned.append(pred)\n",
    "    true_vals_tuned.append(y_h.iloc[i])\n",
    "\n",
    "hybrid_preds_tuned = np.array(hybrid_preds_tuned)\n",
    "true_vals_tuned = np.array(true_vals_tuned)\n",
    "\n",
    "rmse_tuned = np.sqrt(mean_squared_error(true_vals_tuned, hybrid_preds_tuned))\n",
    "mae_tuned = mean_absolute_error(true_vals_tuned, hybrid_preds_tuned)\n",
    "dir_tuned = np.mean(np.sign(hybrid_preds_tuned) == np.sign(true_vals_tuned))\n",
    "\n",
    "print(f\"RMSE (Tuned Hybrid): {rmse_tuned:.6f}\")\n",
    "print(f\"MAE  (Tuned Hybrid): {mae_tuned:.6f}\")\n",
    "print(f\"DIR  (Tuned Hybrid): {dir_tuned:.6f}\")\n",
    "\n",
    "# Optional: Store results for report comparison\n",
    "results_df.loc[\"Tuned_1d_Hybrid\"] = [np.nan, rmse_tuned, np.nan, mae_tuned, np.nan, dir_tuned]\n",
    "\n",
    "print(\"\\n\\n====== Updated Results Summary ======\")\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9f40c-4b66-482b-8f07-fa49c2296d09",
   "metadata": {},
   "source": [
    "## 8. Apply Tuned XGB to Multi-Horizon Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0968112-a799-413b-8a42-0e3c706af999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Evaluating Tuned Model across Multiple Horizons ======\n",
      "\n",
      "---- Tuned XGB Horizon = 1 days ----\n",
      "RMSE: 0.092344, MAE: 0.059767, DIR: 0.478372\n",
      "\n",
      "---- Tuned XGB Horizon = 5 days ----\n",
      "RMSE: 0.091148, MAE: 0.059095, DIR: 0.477041\n",
      "\n",
      "---- Tuned XGB Horizon = 22 days ----\n",
      "RMSE: 0.092339, MAE: 0.059486, DIR: 0.481959\n",
      "\n",
      "---- Tuned XGB Horizon = 66 days ----\n",
      "RMSE: 0.092746, MAE: 0.059803, DIR: 0.471053\n",
      "\n",
      "\n",
      "====== Tuned Multi-Horizon Results ======\n",
      "    RMSE_Tuned  MAE_Tuned  DIR_Tuned\n",
      "1     0.092344   0.059767   0.478372\n",
      "5     0.091148   0.059095   0.477041\n",
      "22    0.092339   0.059486   0.481959\n",
      "66    0.092746   0.059803   0.471053\n",
      "\n",
      "\n",
      "====== UPDATED RESULTS TABLE (All models) ======\n",
      "                  RMSE_GARCH  RMSE_HYBRID  MAE_GARCH  MAE_HYBRID  DIR_GARCH  \\\n",
      "1                   0.090399     0.101633   0.058608    0.066116   0.559796   \n",
      "5                   0.090480     0.096608   0.058617    0.064103   0.558673   \n",
      "22                  0.090532     0.100377   0.058507    0.065766   0.559278   \n",
      "66                  0.091037     0.100059   0.058686    0.065745   0.557895   \n",
      "Tuned_1d_Hybrid          NaN     0.092344        NaN    0.059767        NaN   \n",
      "Tuned_5d_Hybrid          NaN     0.091148        NaN    0.059095        NaN   \n",
      "Tuned_22d_Hybrid         NaN     0.092339        NaN    0.059486        NaN   \n",
      "Tuned_66d_Hybrid         NaN     0.092746        NaN    0.059803        NaN   \n",
      "\n",
      "                  DIR_HYBRID  \n",
      "1                   0.460560  \n",
      "5                   0.482143  \n",
      "22                  0.463918  \n",
      "66                  0.468421  \n",
      "Tuned_1d_Hybrid     0.478372  \n",
      "Tuned_5d_Hybrid     0.477041  \n",
      "Tuned_22d_Hybrid    0.481959  \n",
      "Tuned_66d_Hybrid    0.471053  \n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 8. Apply Tuned XGB to Multi-Horizon Forecasts\n",
    "# ==============================\n",
    "\n",
    "print(\"\\n\\n====== Evaluating Tuned Model across Multiple Horizons ======\")\n",
    "\n",
    "tuned_results = {}\n",
    "\n",
    "for h in [1, 5, 22, 66]:\n",
    "    print(f\"\\n---- Tuned XGB Horizon = {h} days ----\")\n",
    "    y_h = data[target_col].shift(-h).dropna()\n",
    "    X_h = X.loc[y_h.index]\n",
    "    train_size_h = int(len(X_h) * 0.8)\n",
    "\n",
    "    preds_tuned_h = []\n",
    "    true_vals_h = []\n",
    "\n",
    "    for i in range(train_size_h, len(X_h)):\n",
    "        X_train = X_h.iloc[:i]\n",
    "        y_train = y_h.iloc[:i]\n",
    "        X_test = X_h.iloc[i:i+1]\n",
    "\n",
    "        tuned_model = XGBRegressor(\n",
    "            **best_params,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=SEED,\n",
    "            n_jobs=1,\n",
    "            tree_method=\"exact\"\n",
    "        )\n",
    "        tuned_model.fit(X_train, y_train)\n",
    "        pred = tuned_model.predict(X_test)[0]\n",
    "        preds_tuned_h.append(pred)\n",
    "        true_vals_h.append(y_h.iloc[i])\n",
    "\n",
    "    preds_tuned_h = np.array(preds_tuned_h)\n",
    "    true_vals_h = np.array(true_vals_h)\n",
    "\n",
    "    rmse_tuned = np.sqrt(mean_squared_error(true_vals_h, preds_tuned_h))\n",
    "    mae_tuned = mean_absolute_error(true_vals_h, preds_tuned_h)\n",
    "    dir_tuned = np.mean(np.sign(preds_tuned_h) == np.sign(true_vals_h))\n",
    "\n",
    "    tuned_results[h] = [rmse_tuned, mae_tuned, dir_tuned]\n",
    "    print(f\"RMSE: {rmse_tuned:.6f}, MAE: {mae_tuned:.6f}, DIR: {dir_tuned:.6f}\")\n",
    "\n",
    "# Combine results\n",
    "tuned_df = pd.DataFrame(tuned_results, index=[\"RMSE_Tuned\", \"MAE_Tuned\", \"DIR_Tuned\"]).T\n",
    "print(\"\\n\\n====== Tuned Multi-Horizon Results ======\")\n",
    "print(tuned_df)\n",
    "\n",
    "# ==============================\n",
    "# Combine ALL results into one summary table\n",
    "# ==============================\n",
    "\n",
    "# Create new rows\n",
    "tuned_rows = [\"Tuned_5d_Hybrid\", \"Tuned_22d_Hybrid\", \"Tuned_66d_Hybrid\"]\n",
    "tuned_horizons = [5, 22, 66]\n",
    "\n",
    "for row_name, h in zip(tuned_rows, tuned_horizons):\n",
    "    results_df.loc[row_name, \"RMSE_GARCH\"] = np.nan\n",
    "    results_df.loc[row_name, \"MAE_GARCH\"] = np.nan\n",
    "    results_df.loc[row_name, \"DIR_GARCH\"] = np.nan\n",
    "\n",
    "    results_df.loc[row_name, \"RMSE_HYBRID\"] = tuned_df.loc[h, \"RMSE_Tuned\"]\n",
    "    results_df.loc[row_name, \"MAE_HYBRID\"]  = tuned_df.loc[h, \"MAE_Tuned\"]\n",
    "    results_df.loc[row_name, \"DIR_HYBRID\"]  = tuned_df.loc[h, \"DIR_Tuned\"]\n",
    "\n",
    "print(\"\\n\\n====== UPDATED RESULTS TABLE (All models) ======\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68592fd-b5aa-44ea-b02f-4f2b4230efc7",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27acf165-25ad-4b65-a845-4bd14a147a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Tuned Hybrid (5-day cumulative target) ======\n",
      "RMSE: 0.094445, MAE: 0.063975, DIR: 0.930946\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Step 1: Add Medium-Term Features (應該是錯的結果)\n",
    "# ==============================\n",
    "\n",
    "# Add 5-day rolling mean and std of VIX returns (trend + volatility)\n",
    "data[\"vix_ret_mean5\"] = data[\"VIX_return\"].rolling(window=5).mean()\n",
    "data[\"vix_ret_std5\"] = data[\"VIX_return\"].rolling(window=5).std()\n",
    "\n",
    "# Add sentiment momentum (5-day difference)\n",
    "data[\"sent_mom5\"] = data[\"SGIXSENT_z\"].diff(5)\n",
    "\n",
    "# Add S&P 500 5-day return (market momentum)\n",
    "data[\"sp500_ret5\"] = np.log(data[\"^GSPC\"] / data[\"^GSPC\"].shift(5))\n",
    "\n",
    "# Re-drop missing\n",
    "data = data.dropna().copy()\n",
    "\n",
    "# Redefine feature set\n",
    "feature_cols_extended = feature_cols + [\"vix_ret_mean5\", \"vix_ret_std5\", \"sent_mom5\", \"sp500_ret5\"]\n",
    "X_ext = data[feature_cols_extended]\n",
    "y_ext = data[target_col]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Redefine 5-day target as cumulative return\n",
    "# ==============================\n",
    "\n",
    "data[\"VIX_return_5d\"] = np.log(data[\"VIX\"] / data[\"VIX\"].shift(5))\n",
    "data = data.dropna(subset=[\"VIX_return_5d\"])\n",
    "\n",
    "X_5d = data[feature_cols_extended]\n",
    "y_5d = data[\"VIX_return_5d\"]\n",
    "\n",
    "# ==============================\n",
    "# Step 3: Train Tuned Model on 5-day Horizon (Cumulative Target)\n",
    "# ==============================\n",
    "\n",
    "train_size_5d = int(len(X_5d) * 0.8)\n",
    "hybrid_preds_5d = []\n",
    "true_vals_5d = []\n",
    "\n",
    "for i in range(train_size_5d, len(X_5d)):\n",
    "    X_train = X_5d.iloc[:i]\n",
    "    y_train = y_5d.iloc[:i]\n",
    "    X_test = X_5d.iloc[i:i+1]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"exact\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)[0]\n",
    "\n",
    "    hybrid_preds_5d.append(pred)\n",
    "    true_vals_5d.append(y_5d.iloc[i])\n",
    "\n",
    "hybrid_preds_5d = np.array(hybrid_preds_5d)\n",
    "true_vals_5d = np.array(true_vals_5d)\n",
    "\n",
    "rmse_5d = np.sqrt(mean_squared_error(true_vals_5d, hybrid_preds_5d))\n",
    "mae_5d = mean_absolute_error(true_vals_5d, hybrid_preds_5d)\n",
    "dir_5d = np.mean(np.sign(hybrid_preds_5d) == np.sign(true_vals_5d))\n",
    "\n",
    "print(\"\\n\\n====== Tuned Hybrid (5-day cumulative target) ======\")\n",
    "print(f\"RMSE: {rmse_5d:.6f}, MAE: {mae_5d:.6f}, DIR: {dir_5d:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851840a-2826-434b-b97b-28e916e0fbee",
   "metadata": {},
   "source": [
    "結果解釋:  \n",
    "\n",
    "RMSE 上升只是因為「5日累積報酬」的數值幅度比單日報酬大；\n",
    "但方向正確率（DIR）接近 0.93 表示你的模型成功捕捉趨勢方向，\n",
    "這對「5-day 投資策略」或 volatility trading 來說比 RMSE 更有意義。\n",
    "\n",
    "\n",
    "原本的 shift(-5) 是在預測第 5 天那一點的值，噪音高、難學。\n",
    "\n",
    "現在的 log(VIX_t / VIX_t-5) 是 5 天的趨勢，信號更穩。\n",
    "\n",
    "XGBoost 非常擅長學習這種「跨期趨勢」特徵，\n",
    "所以它會犧牲一點誤差大小（RMSE）去換取方向判斷的準確度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c32d3c34-2e44-4d3b-a5b4-f574c52dee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9813018873184329\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(hybrid_preds_5d, true_vals_5d)[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa32423-b6d5-4455-8a44-d901592eac2c",
   "metadata": {},
   "source": [
    "→ 若相關係數也高 (>0.6)，代表模型不只是方向對，而是真的跟趨勢對齊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4adf6353-3b05-426a-a9e5-bb195e6e09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Tuned Hybrid (5-day forward target) ======\n",
      "RMSE: 0.181985, MAE: 0.129862, DIR: 0.575835\n",
      "corr: 0.32493295720188803\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Step 1: Add Medium-Term Features (有修正過了但我還沒仔細想過)\n",
    "# ==============================\n",
    "\n",
    "# Add 5-day rolling mean and std of VIX returns (trend + volatility)\n",
    "data[\"vix_ret_mean5\"] = data[\"VIX_return\"].rolling(window=5).mean()\n",
    "data[\"vix_ret_std5\"] = data[\"VIX_return\"].rolling(window=5).std()\n",
    "\n",
    "# Add sentiment momentum (5-day difference)\n",
    "data[\"sent_mom5\"] = data[\"SGIXSENT_z\"].diff(5)\n",
    "\n",
    "# Add S&P 500 5-day return (market momentum)\n",
    "data[\"sp500_ret5\"] = np.log(data[\"^GSPC\"] / data[\"^GSPC\"].shift(5))\n",
    "\n",
    "# Re-drop missing\n",
    "data = data.dropna().copy()\n",
    "\n",
    "# Redefine feature set\n",
    "feature_cols_extended = feature_cols + [\"vix_ret_mean5\", \"vix_ret_std5\", \"sent_mom5\", \"sp500_ret5\"]\n",
    "X_ext = data[feature_cols_extended]\n",
    "y_ext = data[target_col] \n",
    "\n",
    "# ---------- 修正版 Step 2 & 3 (5-day forward forecast, no leakage) ----------\n",
    "\n",
    "# Step 2 (修正): 建立真正的 5-day forward target（t -> t+5）\n",
    "h = 5\n",
    "# y_forward[t] = log(VIX_{t+h} / VIX_t)\n",
    "data[\"VIX_return_forward_5d\"] = np.log(data[\"VIX\"].shift(-h) / data[\"VIX\"])\n",
    "# drop rows where target is NaN (因為尾端不夠 h 天)\n",
    "data_f5 = data.dropna(subset=[\"VIX_return_forward_5d\"]).copy()\n",
    "\n",
    "# features (確保 features 是由 <= t 的資訊計算)\n",
    "# vix_ret_mean5/vix_ret_std5/sp500_ret5/sent_mom5 已在 earlier 建好，且為截至 t 的資訊\n",
    "X_5d = data_f5[feature_cols_extended]\n",
    "y_5d = data_f5[\"VIX_return_forward_5d\"]\n",
    "\n",
    "# Step 3 (修正): rolling expanding forecast to predict t+5 using info up to t\n",
    "train_size_5d = int(len(X_5d) * 0.8)\n",
    "hybrid_preds_5d = []\n",
    "true_vals_5d = []\n",
    "\n",
    "for i in range(train_size_5d, len(X_5d)):\n",
    "    # training uses samples up to i-1 (i.e., indices < i)\n",
    "    X_train = X_5d.iloc[:i]\n",
    "    y_train = y_5d.iloc[:i]\n",
    "    # X_test is features at time t_i, whose y is VIX_{t_i + 5}\n",
    "    X_test = X_5d.iloc[i:i+1]\n",
    "\n",
    "    # train and predict\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"exact\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)[0]\n",
    "\n",
    "    hybrid_preds_5d.append(pred)\n",
    "    true_vals_5d.append(y_5d.iloc[i])\n",
    "\n",
    "hybrid_preds_5d = np.array(hybrid_preds_5d)\n",
    "true_vals_5d = np.array(true_vals_5d)\n",
    "\n",
    "rmse_5d = np.sqrt(mean_squared_error(true_vals_5d, hybrid_preds_5d))\n",
    "mae_5d = mean_absolute_error(true_vals_5d, hybrid_preds_5d)\n",
    "dir_5d = np.mean(np.sign(hybrid_preds_5d) == np.sign(true_vals_5d))\n",
    "\n",
    "print(\"\\n\\n====== Tuned Hybrid (5-day forward target) ======\")\n",
    "print(f\"RMSE: {rmse_5d:.6f}, MAE: {mae_5d:.6f}, DIR: {dir_5d:.6f}\")\n",
    "print(\"corr:\", np.corrcoef(hybrid_preds_5d, true_vals_5d)[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6989a-555e-4a01-a08a-6b129fcc0def",
   "metadata": {},
   "source": [
    "## 9. Tune XGB specifically for 5-day ahead forecast (shift-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "358a58f0-b30c-4744-b106-f00a378547fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== 5-Day Ahead Tuning (shift-based target) ======\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "\n",
      "Best params for 5-day horizon:\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "\n",
      "====== Tuned 5-day Results (shift-based) ======\n",
      "RMSE: 0.092046\n",
      "MAE : 0.059641\n",
      "DIR : 0.467866\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 9. Tune XGB specifically for 5-day ahead forecast (shift-based)\n",
    "# ==============================\n",
    "\n",
    "print(\"\\n\\n====== 5-Day Ahead Tuning (shift-based target) ======\")\n",
    "\n",
    "# ---- Step 1: Create 5-day forecast target ----\n",
    "h = 5\n",
    "y_5 = data[target_col].shift(-h).dropna()\n",
    "X_5 = X.loc[y_5.index]\n",
    "\n",
    "# ---- Step 2: Split train/test the same way ----\n",
    "train_size_5 = int(len(X_5) * 0.8)\n",
    "X_train_5 = X_5.iloc[:train_size_5]\n",
    "y_train_5 = y_5.iloc[:train_size_5]\n",
    "\n",
    "# ---- Step 3: Hyperparameter Grid ----\n",
    "param_grid_5 = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_base_5 = XGBRegressor(objective='reg:squarederror', random_state=SEED)\n",
    "\n",
    "grid_search_5 = GridSearchCV(\n",
    "    xgb_base_5,\n",
    "    param_grid_5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_5.fit(X_train_5, y_train_5)\n",
    "best_params_5 = grid_search_5.best_params_\n",
    "\n",
    "print(\"\\nBest params for 5-day horizon:\")\n",
    "print(best_params_5)\n",
    "\n",
    "# ---- Step 4: Evaluate tuned model via expanding rolling forecast ----\n",
    "\n",
    "pred_5_tuned = []\n",
    "true_5_vals = []\n",
    "\n",
    "for i in range(train_size_5, len(X_5)):\n",
    "    X_train_win = X_5.iloc[:i]\n",
    "    y_train_win = y_5.iloc[:i]\n",
    "    X_test_win  = X_5.iloc[i:i+1]\n",
    "\n",
    "    model_5 = XGBRegressor(\n",
    "        **best_params_5,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"exact\"\n",
    "    )\n",
    "    model_5.fit(X_train_win, y_train_win)\n",
    "    forecast = model_5.predict(X_test_win)[0]\n",
    "\n",
    "    pred_5_tuned.append(forecast)\n",
    "    true_5_vals.append(y_5.iloc[i])\n",
    "\n",
    "pred_5_tuned = np.array(pred_5_tuned)\n",
    "true_5_vals = np.array(true_5_vals)\n",
    "\n",
    "# ---- Step 5: Metrics ----\n",
    "rmse_5_tuned = np.sqrt(mean_squared_error(true_5_vals, pred_5_tuned))\n",
    "mae_5_tuned  = mean_absolute_error(true_5_vals, pred_5_tuned)\n",
    "dir_5_tuned  = np.mean(np.sign(true_5_vals) == np.sign(pred_5_tuned))\n",
    "\n",
    "print(\"\\n====== Tuned 5-day Results (shift-based) ======\")\n",
    "print(f\"RMSE: {rmse_5_tuned:.6f}\")\n",
    "print(f\"MAE : {mae_5_tuned:.6f}\")\n",
    "print(f\"DIR : {dir_5_tuned:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1df832eb-cb9e-4336-bdb0-cbffccb91a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Hybrid Model v2 (5-day ahead, no leakage, medium-term features) ======\n",
      "RMSE: 0.179813\n",
      "MAE : 0.126259\n",
      "DIR : 0.611399\n",
      "Corr: 0.337989\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# NEW: Hybrid Model for 5-Day Ahead Forecast (Leakage-Free)\n",
    "# ==============================\n",
    "\n",
    "print(\"\\n\\n====== Hybrid Model v2 (5-day ahead, no leakage, medium-term features) ======\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 1: Construct medium-term features (no leakage)\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = data.copy()   # 原本 hybrid 用的資料（已對齊 GARCH，無缺漏）\n",
    "\n",
    "# 5-day rolling mean/std of returns\n",
    "df[\"vix_ret_mean5\"] = df[\"VIX_return\"].rolling(5).mean()\n",
    "df[\"vix_ret_std5\"]  = df[\"VIX_return\"].rolling(5).std()\n",
    "\n",
    "# 10-day rolling mean/std (new)\n",
    "df[\"vix_ret_mean10\"] = df[\"VIX_return\"].rolling(10).mean()\n",
    "df[\"vix_ret_std10\"]  = df[\"VIX_return\"].rolling(10).std()\n",
    "\n",
    "# sentiment momentum (past 5 days)\n",
    "df[\"sent_mom5\"] = df[\"SGIXSENT_z\"].diff(5)\n",
    "\n",
    "# SP500 5-day return (market trend)\n",
    "df[\"sp500_ret5\"] = np.log(df[\"^GSPC\"] / df[\"^GSPC\"].shift(5))\n",
    "\n",
    "# 重新 drop missing\n",
    "df = df.dropna().copy()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 2: Build 5-day ahead target (forward target)\n",
    "# -------------------------------------------------\n",
    "\n",
    "h = 5\n",
    "df[\"VIX_forward_5d\"] = np.log(df[\"VIX\"].shift(-h) / df[\"VIX\"])\n",
    "df_fwd5 = df.dropna(subset=[\"VIX_forward_5d\"]).copy()\n",
    "\n",
    "y_5d = df_fwd5[\"VIX_forward_5d\"]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 3: Define extended feature set\n",
    "# -------------------------------------------------\n",
    "\n",
    "feature_cols_v2 = feature_cols + [\n",
    "    \"vix_ret_mean5\", \"vix_ret_std5\",\n",
    "    \"vix_ret_mean10\", \"vix_ret_std10\",\n",
    "    \"sent_mom5\", \"sp500_ret5\"\n",
    "]\n",
    "\n",
    "X_5d = df_fwd5[feature_cols_v2]\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 4: Rolling expanding forecasting (same as hybrid)\n",
    "# -------------------------------------------------\n",
    "\n",
    "train_size_5d = int(len(X_5d) * 0.8)\n",
    "preds_5d_v2 = []\n",
    "true_vals_5d_v2 = []\n",
    "\n",
    "for i in range(train_size_5d, len(X_5d)):\n",
    "    X_train = X_5d.iloc[:i]\n",
    "    y_train = y_5d.iloc[:i]\n",
    "    X_test  = X_5d.iloc[i:i+1]\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        **best_params,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"exact\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)[0]\n",
    "\n",
    "    preds_5d_v2.append(pred)\n",
    "    true_vals_5d_v2.append(y_5d.iloc[i])\n",
    "\n",
    "preds_5d_v2 = np.array(preds_5d_v2)\n",
    "true_vals_5d_v2 = np.array(true_vals_5d_v2)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Step 5: Compute metrics\n",
    "# -------------------------------------------------\n",
    "\n",
    "rmse_v2 = np.sqrt(mean_squared_error(true_vals_5d_v2, preds_5d_v2))\n",
    "mae_v2 =  mean_absolute_error(true_vals_5d_v2, preds_5d_v2)\n",
    "dir_v2 =  np.mean(np.sign(preds_5d_v2) == np.sign(true_vals_5d_v2))\n",
    "corr_v2 = np.corrcoef(preds_5d_v2, true_vals_5d_v2)[0,1]\n",
    "\n",
    "print(f\"RMSE: {rmse_v2:.6f}\")\n",
    "print(f\"MAE : {mae_v2:.6f}\")\n",
    "print(f\"DIR : {dir_v2:.6f}\")\n",
    "print(f\"Corr: {corr_v2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb04bd1-c0f8-47c7-b3cd-c79ed9c82783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
